{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this piece of code converts the raw csv file to processed input\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "data = pd.read_csv('earbud_reviews.csv', encoding='utf-8')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "a = 0\n",
    "LL = {} # dict to store num of appearance\n",
    "punctuation_string = string.punctuation # store the punctuation\n",
    "\n",
    "# the following piece computes the number of appearance of each word in the whole text and store in a dict\n",
    "for line in data.values:\n",
    "    stri = str(line[13]) # this is the review body\n",
    "    stri = stri.replace(\"  \", \" \") # to eliminate some double space appear together\n",
    "    L = nltk.word_tokenize(stri) # tokenlize \n",
    "    for word in L:\n",
    "        if word.isalnum(): # only keep the words that contain only letters and numbers\n",
    "            stemmed = porter.stem(word.lower()) # stem the word\n",
    "            if stemmed in LL:\n",
    "                LL[stemmed] += 1 # if the word in alr in dict, add the number of appearance\n",
    "            else:\n",
    "                LL[stemmed] = 0 # else number of appearance is 0\n",
    "            #LL.append(porter.stem(word)) # append each word of each review body to LL\n",
    "    a += 1 \n",
    "    print(a) # just to show the progress\n",
    "\n",
    "# store the number of appearance into a file for further use\n",
    "with open('temp.txt','a+', encoding='utf-8') as f:\n",
    "    for key in LL.keys():\n",
    "        f.write(str(key) + \" \" + str(LL[key]) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# store the \n",
    "with open('earbud.txt','a+', encoding='utf-8') as f: # store the data in this file\n",
    "    with open('new_vote.txt','a+', encoding = 'utf-8') as ff:\n",
    "        with open('o.txt', 'a+', encoding = 'utf-8') as fff:\n",
    "            temp = 0 \n",
    "            for line in data.values:\n",
    "                stri = str(line[13]) # the original review \n",
    "                L = nltk.word_tokenize(stri)\n",
    "                LLL = [word.lower() for word in L if word.isalnum()]\n",
    "                if len(LLL) > 1: # if the processed review has more than 1 word inside\n",
    "                    filtered_words = []\n",
    "                    for word in LLL:\n",
    "                        stemmed = porter.stem(word)\n",
    "                        if (stemmed not in stopwords.words('english')) and (stemmed in LL) and (LL[stemmed] >= 10):\n",
    "                        # only keep the words that are not stopwords in english and appears more than 10 times in the whole corpus\n",
    "                            filtered_words.append(stemmed)\n",
    "                    new = \" \".join(filtered_words)\n",
    "\n",
    "                    t = str(line[1]) + \"_\" + str(line[2]) + \" \" + str(line[7]) + \" \" + new\n",
    "                    # line[1] is the customer id\n",
    "                    # line [2] is the review id\n",
    "                    # line[7] is the actual rating\n",
    "                    # t should look like this:\n",
    "                    # 13000908_R27F4OF4BIA4LU 2 last long stop work within year\n",
    "\n",
    "                    processed_content = t.split(\" \", 2)[2] \n",
    "\n",
    "                    if processed_content != \"\" and processed_content != \" \" and processed_content != \"\\t\": # if processed_content actually contains comething\n",
    "                        f.write(t + '\\n')\n",
    "                        fff.write(str(line[13])+\"\\n\") # record the original content into a new file\n",
    "                        ff.write((str(line[8]) + \" \" + str(int(line[9]) - int(line[8]))+ \"\\n\")) # get the up and down vote and write to new file\n",
    "\n",
    "                        temp+=1\n",
    "                        print(temp) # to show the progress\n",
    "            fff.close()\n",
    "        ff.close()\n",
    "    f.close()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write all the data into csv file\n",
    "\n",
    "#customer id, review id, actual rating, predicted rating, processed content, original content, up vote, down vote, prob 1-10, entropy, review length\n",
    "\n",
    "# earbud_reviews.txt: customer_id review_id\n",
    "import csv  \n",
    "import nltk\n",
    "from scipy.stats import entropy\n",
    "file = open(\"AIR_document_topic\") # output file\n",
    "\n",
    "LL = []\n",
    "header = ['Cutomer ID', 'Review ID', 'Actual Rating', 'Predicted Rating', 'Processed Content', 'Original Content', 'Up Vote', 'Down Vote',\n",
    "    'prob_a1', 'prob_a2', 'prob_a3', 'prob_a4', 'prob_a5', 'prob_a6', 'prob_a7', 'prob_a8', 'prob_a9', 'prob_a10', 'Entropy', 'Review Length']\n",
    "\n",
    "LL.append(header)\n",
    "topic_score = file.readlines() # extract data\n",
    "\n",
    "\n",
    "with open(\"filtered_tokenlized.txt\") as file2:\n",
    "    info = lines = [line.rstrip('\\n') for line in file2]\n",
    "    # info is all the lines\n",
    "\n",
    "file3 = open(\"AIR_score\") # predicted score by the algo\n",
    "f3 = file3.readlines()\n",
    "\n",
    "file4 = open(\"o.txt\") \n",
    "# this is the file containing the otiginal content\n",
    "f4 = file4.readlines()\n",
    "\n",
    "file5 = open(\"new_vote.txt\")\n",
    "# file containing up and down votes\n",
    "f5 = file5.readlines()\n",
    "\n",
    "for i in range(630526): # number of reviews after filter\n",
    "    L = []\n",
    "    temp = topic_score[i].split(\" \")[0].split(\"\\t\") # this is each score\n",
    "    t = info[i] # info for ith line\n",
    "    tempp = [int(x) for x in temp] # convert str to int\n",
    "    summation = sum(tempp) # sum of all the numbers \n",
    "\n",
    "    processed_content = t.split(\" \", 2)[2]\n",
    "\n",
    "    score = f3[i][:-1].split(\"\\t\")\n",
    "    scoree = [float(x) for x in score][1:]\n",
    "\n",
    "    predicted_score = sum(scoree)/10\n",
    "\n",
    "    cus_review_id = t.split(\" \", 1)[0].split(\"_\")\n",
    "\n",
    "    original_content = f4[i].rstrip(\"\\n\")\n",
    "\n",
    "    up_down = f5[i].rstrip(\"\\n\").split(\" \")\n",
    "    upvotes = int(up_down[0])\n",
    "    downvotes = int(up_down[1])\n",
    "\n",
    "    ent = entropy(list(tempp), base=2) # get the entropy\n",
    "\n",
    "    words = nltk.word_tokenize(original_content)\n",
    "    words=[word for word in words if word.isalnum()]\n",
    "\n",
    "\n",
    "    L.append(cus_review_id[0])\n",
    "    L.append(cus_review_id[1])\n",
    "    L.append(score[0])\n",
    "    L.append(predicted_score)\n",
    "    L.append(processed_content)\n",
    "    # original content\n",
    "    L.append(original_content)\n",
    "    L.append(upvotes)\n",
    "    L.append(downvotes)\n",
    "\n",
    "    for num in tempp:\n",
    "        # convert the probability to %\n",
    "        L.append(\"%.2f\" % (num/summation*100) + \"%\" + \"  \")\n",
    "\n",
    "    L.append(ent) # entropy\n",
    "    L.append(len(words)) # review length\n",
    "\n",
    "    LL.append(L)\n",
    "    print(i) # show the progress\n",
    "\n",
    "print(len(LL))\n",
    "\n",
    "\n",
    "with open('new.csv', 'a+', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    writer.writerows(LL)\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1 (v3.9.1:1e5d33e9b9, Dec  7 2020, 12:44:01) \n[Clang 12.0.0 (clang-1200.0.32.27)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
