{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this piece of code converts the raw csv file to processed input\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#读取news_data.csv，保存到新建的news_data.txt中\n",
    "data = pd.read_csv('earbud_reviews.csv', encoding='utf-8')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "a = 0\n",
    "LL = {} # dict to store num of appearance\n",
    "punctuation_string = string.punctuation # store the punctuation\n",
    "'''\n",
    "for line in data.values:\n",
    "    stri = str(line[13]) # this is the review body\n",
    "    stri = stri.replace(\"  \", \" \")\n",
    "    L = nltk.word_tokenize(stri) # tokenlize \n",
    "    #L = stri.split(\" \") # split the review body by space\n",
    "    for word in L:\n",
    "        if word.isalnum():\n",
    "            stemmed = porter.stem(word.lower()) # stem the word\n",
    "            if stemmed in LL:\n",
    "                LL[stemmed] += 1\n",
    "            else:\n",
    "                LL[stemmed] = 0\n",
    "            #LL.append(porter.stem(word)) # append each word of each review body to LL\n",
    "    a += 1\n",
    "    print(a)\n",
    "    \n",
    "#print(LL)\n",
    "with open('temp.txt','a+', encoding='utf-8') as f:\n",
    "    for key in LL.keys():\n",
    "        f.write(str(key) + \" \" + str(LL[key]) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "'''\n",
    "with open('temp.txt') as f:\n",
    "    dataa = f.readlines()\n",
    "    for line in dataa:\n",
    "        line = line.split(\" \")\n",
    "        LL[line[0]] = int(line[1])\n",
    "    f.close()\n",
    "\n",
    "\n",
    "with open('earbud.txt','a+', encoding='utf-8') as f: # store the data in this file\n",
    "    with open('new_vote.txt','a+', encoding = 'utf-8') as ff:\n",
    "        with open('o.txt', 'a+', encoding = 'utf-8') as fff:\n",
    "            temp = 0 \n",
    "            for line in data.values:\n",
    "                #if temp < 100:\n",
    "\n",
    "                stri = str(line[13])\n",
    "                '''\n",
    "                for i in punctuation_string:\n",
    "                    stri = stri.replace(i, '') # replace punctuation with nothing\n",
    "                stri = stri.replace(\"  \", \" \")\n",
    "                stri = stri.lower() # convert to lower case\n",
    "                L = stri.split(\" \") # split by space\n",
    "                '''\n",
    "                L = nltk.word_tokenize(stri)\n",
    "                LLL = [word.lower() for word in L if word.isalnum()]\n",
    "                if len(LLL) > 1:\n",
    "                    filtered_words = []\n",
    "                    for word in LLL:\n",
    "                        stemmed = porter.stem(word)\n",
    "                        if (stemmed not in stopwords.words('english')) and (stemmed in LL) and (LL[stemmed] >= 10):\n",
    "                            filtered_words.append(stemmed)\n",
    "                    #filtered_words = [porter.stem(word) for word in L if (porter.stem(word) not in stopwords.words('english')) and (LL.count(word)) >= 10] # keep only if appear more than 10 times and not stop words\n",
    "                    new = \" \".join(filtered_words)\n",
    "                    #f.write(str(line[1]) + \" \" + str(line[2]) + \" \" + new + \" \" + stri + \" \" + str(line[8]) + \" \" + str(int(line[9]) - int(line[8])) + '\\n')\n",
    "                    #if new.isalnum():\n",
    "\n",
    "                    t = str(line[1]) + \"_\" + str(line[2]) + \" \" + str(line[7]) + \" \" + new\n",
    "                    processed_content = t.split(\" \", 2)[2]\n",
    "                    if processed_content != \"\" and processed_content != \" \" and processed_content != \"\\t\":\n",
    "                        f.write(t + '\\n')\n",
    "                        fff.write(str(line[13])+\"\\n\")\n",
    "                        ff.write((str(line[8]) + \" \" + str(int(line[9]) - int(line[8]))+ \"\\n\"))\n",
    "                    \n",
    "                        # line[1] is customer_id \n",
    "                        # line[2] is review_id\n",
    "                        temp+=1\n",
    "                        print(temp)\n",
    "            fff.close()\n",
    "        ff.close()\n",
    "    f.close()\n",
    "            \n",
    "        #temp += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all the data into csv file\n",
    "\n",
    "#customer id, review id, actual rating, predicted rating, processed content, original content, up vote, down vote, prob 1-10, entropy, review length\n",
    "\n",
    "# earbud_reviews.txt: customer_id review_id\n",
    "import csv  \n",
    "import nltk\n",
    "from scipy.stats import entropy\n",
    "file = open(\"AIR_document_topic\") # output file\n",
    "\n",
    "LL = []\n",
    "header = ['Cutomer ID', 'Review ID', 'Actual Rating', 'Predicted Rating', 'Processed Content', 'Original Content', 'Up Vote', 'Down Vote',\n",
    "    'prob_a1', 'prob_a2', 'prob_a3', 'prob_a4', 'prob_a5', 'prob_a6', 'prob_a7', 'prob_a8', 'prob_a9', 'prob_a10', 'Entropy', 'Review Length']\n",
    "\n",
    "LL.append(header)\n",
    "topic_score = file.readlines() # extract data\n",
    "\n",
    "\n",
    "with open(\"filtered_tokenlized.txt\") as file2:\n",
    "    info = lines = [line.rstrip('\\n') for line in file2]\n",
    "    # info is all the lines\n",
    "\n",
    "file3 = open(\"AIR_score\") # predicted score by the algo\n",
    "\n",
    "f3 = file3.readlines()\n",
    "\n",
    "file4 = open(\"o.txt\")\n",
    "f4 = file4.readlines()\n",
    "\n",
    "file5 = open(\"new_vote.txt\")\n",
    "f5 = file5.readlines()\n",
    "\n",
    "for i in range(630526): # number of reviews after filter\n",
    "    L = []\n",
    "    temp = topic_score[i].split(\" \")[0].split(\"\\t\") # this is each score\n",
    "    t = info[i] # info for ith line\n",
    "    tempp = [int(x) for x in temp] # convert str to int\n",
    "    summation = sum(tempp) # sum of all the numbers \n",
    "\n",
    "    processed_content = t.split(\" \", 2)[2]\n",
    "\n",
    "\n",
    "    \n",
    "    score = f3[i][:-1].split(\"\\t\")\n",
    "    scoree = [float(x) for x in score][1:]\n",
    "\n",
    "    predicted_score = sum(scoree)/10\n",
    "\n",
    "    cus_review_id = t.split(\" \", 1)[0].split(\"_\")\n",
    "\n",
    "    original_content = f4[i].rstrip(\"\\n\")\n",
    "\n",
    "    up_down = f5[i].rstrip(\"\\n\").split(\" \")\n",
    "    upvotes = int(up_down[0])\n",
    "    downvotes = int(up_down[1])\n",
    "\n",
    "    ent = entropy(list(tempp), base=2)\n",
    "\n",
    "    words = nltk.word_tokenize(original_content)\n",
    "    words=[word for word in words if word.isalnum()]\n",
    "\n",
    "    \n",
    "\n",
    "    L.append(cus_review_id[0])\n",
    "    L.append(cus_review_id[1])\n",
    "    L.append(score[0])\n",
    "    L.append(predicted_score)\n",
    "    L.append(processed_content)\n",
    "    # original content\n",
    "    L.append(original_content)\n",
    "    L.append(upvotes)\n",
    "    L.append(downvotes)\n",
    "\n",
    "    for num in tempp:\n",
    "        L.append(\"%.2f\" % (num/summation*100) + \"%\" + \"  \")\n",
    "\n",
    "    L.append(ent)\n",
    "    L.append(len(words)) # review length\n",
    "\n",
    "    LL.append(L)\n",
    "    print(i)\n",
    "print(len(LL))\n",
    "\n",
    "\n",
    "with open('countries.csv', 'a+', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    writer.writerows(LL)\n",
    "\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 630526 entries, 0 to 630525\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   Cutomer ID         630526 non-null  int64  \n",
      " 1   Review ID          630526 non-null  object \n",
      " 2   Actual Rating      630526 non-null  float64\n",
      " 3   Predicted Rating   630526 non-null  float64\n",
      " 4   Processed Content  630526 non-null  object \n",
      " 5   Original Content   630526 non-null  object \n",
      " 6   Up Vote            630526 non-null  int64  \n",
      " 7   Down Vote          630526 non-null  int64  \n",
      " 8   prob_a1            630526 non-null  object \n",
      " 9   prob_a2            630526 non-null  object \n",
      " 10  prob_a3            630526 non-null  object \n",
      " 11  prob_a4            630526 non-null  object \n",
      " 12  prob_a5            630526 non-null  object \n",
      " 13  prob_a6            630526 non-null  object \n",
      " 14  prob_a7            630526 non-null  object \n",
      " 15  prob_a8            630526 non-null  object \n",
      " 16  prob_a9            630526 non-null  object \n",
      " 17  prob_a10           630526 non-null  object \n",
      " 18  Entropy            630526 non-null  float64\n",
      " 19  Review Length      630526 non-null  int64  \n",
      "dtypes: float64(3), int64(4), object(13)\n",
      "memory usage: 96.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('new.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below are some pieces of code to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to upload file\n",
    "\n",
    "# scp Desktop/earbud_reviews/filtered_tokenlized.txt sunyt@theorymind.d2.comp.nus.edu.sg:/home/sunyt/AIR/AIR/data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '24\t54\t6\t3\t63\t9\t54'\n",
    "temp = test.split(\" \")[0].split(\"\\t\")\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"earbud_reviews.txt\") as file2:\n",
    "    info = lines = [line.rstrip('\\n') for line in file2]\n",
    "    infoo = info[1].split(\" \", 2)[2]\n",
    "    print(infoo)\n",
    "    file2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '2.0\t2.271\t5.142\t2.343\t2.463\t1.605\t4.714\t0.657'\n",
    "temp = test[:-1].split(\"\\t\")\n",
    "print(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "data = pd.read_csv('earbud_reviews.csv', encoding='utf-8')\n",
    "punctuation_string = string.punctuation\n",
    "\n",
    "with open('origin.txt','a+', encoding='utf-8') as f: \n",
    "    temp = 0 \n",
    "    for line in data.values:\n",
    "            stri = str(line[13])\n",
    "            for i in punctuation_string:\n",
    "                stri = stri.replace(i, '') # replace punctuation with nothing\n",
    "            stri = stri.replace(\"  \", \" \")\n",
    "            stri = stri.lower() # convert to lower case\n",
    "            L = stri.split(\" \") # split by space\n",
    "            if len(L) > 1:\n",
    "                f.write(stri + \"\\n\")\n",
    "                temp+=1\n",
    "                print(temp)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"earbud_reviews\") as file2:\n",
    "    file = open(\"AIR_document_topic\") # output file\n",
    "\n",
    "    topic_score = file.readlines() # extract data\n",
    "    temp = 0\n",
    "    info = [line.rstrip('\\n') for line in file2]\n",
    "    \n",
    "    for i in range(630929):\n",
    "        t = info[i]\n",
    "        processed_content = t.split(\" \", 2)[2]\n",
    "        processed_content = processed_content.split(\" \")\n",
    "        tempp = topic_score[i].split(\" \")[0].split(\"\\t\") # this is each score\n",
    "        temppp = [int(x) for x in tempp] # convert str to int\n",
    "        summation = sum(temppp) # sum of all the numbers \n",
    "        if len(processed_content) != summation:\n",
    "            print(t)\n",
    "            print(topic_score[i])\n",
    "            print(summation)\n",
    "            print(len(processed_content))\n",
    "            break\n",
    "    file2.close()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('earbud_reviews.csv', encoding='utf-8').values\n",
    "#print(data)\n",
    "with open(\"earbud.txt\") as file2:\n",
    "    with open(\"filtered_votes.txt\", 'a+', encoding='utf-8') as file3:\n",
    "        with open('filtered_tokenlized.txt','a+', encoding='utf-8') as f: \n",
    "            temp = 0\n",
    "            info = [line.rstrip('\\n') for line in file2]\n",
    "            for i in range(630651):\n",
    "                t = info[i]\n",
    "                processed_content = t.split(\" \", 2)[2]\n",
    "                if processed_content != \"\" and processed_content != \" \" and processed_content != \"\\t\":\n",
    "                    f.write(info[i] + \"\\n\")\n",
    "                    file3.write(str(data[i][8]) + \" \" + str(int(data[i][9]) - int(data[i][8]))+ \"\\n\")\n",
    "                    temp+=1\n",
    "                print(temp)\n",
    "            f.close()\n",
    "        file3.close()\n",
    "    file2.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stri = \"Simple hanger; clamp is strong, with a pad; hanger swivels to accommodate many possible attachment locations\"\n",
    "\n",
    "L = nltk.word_tokenize(stri)\n",
    "print(L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('earbud_reviews.csv', encoding='utf-8')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "a = 0\n",
    "LL = {} # dict to store num of appearance\n",
    "punctuation_string = string.punctuation # store the punctuation\n",
    "for line in data.values:\n",
    "    stri = str(line[13]) # this is the review body\n",
    "    stri = stri.replace(\"  \", \" \")\n",
    "    L = nltk.word_tokenize(stri) # tokenlize \n",
    "    LL = [word for word in L if word not in punctuation_string]\n",
    "    print(LL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file4 = open(\"filtered_origin.txt\")\n",
    "f4 = file4.readlines()\n",
    "\n",
    "L = []\n",
    "for i in range(100):\n",
    "    L.append(f4[i])\n",
    "\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this piece of code converts the raw csv file to processed input\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#读取news_data.csv，保存到新建的news_data.txt中\n",
    "data = pd.read_csv('earbud_reviews.csv', encoding='utf-8')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "a = 0\n",
    "punctuation_string = string.punctuation # store the punctuation\n",
    "\n",
    "    \n",
    "file2 = open(\"filtered_tokenlized.txt\", \"r\")\n",
    "dataa = file2.readlines()\n",
    "\n",
    "\n",
    "with open('earbud.txt','a+', encoding='utf-8') as f: # store the data in this file\n",
    "    with open(\"new_vote.txt\",'a+', encoding='utf-8') as ff:\n",
    "        temp = 0 \n",
    "        #if temp < 100:\n",
    "        for linee in dataa:\n",
    "            cus_review_id = linee.split(\" \", 1)[0].split(\"_\")\n",
    "            cus_id = cus_review_id[0]\n",
    "            review_id = cus_review_id[1]\n",
    "\n",
    "            for line in data.values:\n",
    "\n",
    "                if line[0] == cus_id and line[1] == review_id:\n",
    "                    f.write(line[13])\n",
    "                    ff.write((str(data[i][8]) + \" \" + str(int(data[i][9]) - int(data[i][8]))+ \"\\n\"))\n",
    "\n",
    "\n",
    "                    temp+=1\n",
    "                    print(temp)\n",
    "                    continue\n",
    "    \n",
    "\n",
    "file2.close()  \n",
    "        #temp += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"2781942_R1DGA6UQIVLKZ7 5 alll good\\n\"\n",
    "print(line.split(\" \", 1)[0].split(\"_\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (v3.9.1:1e5d33e9b9, Dec  7 2020, 12:44:01) \n[Clang 12.0.0 (clang-1200.0.32.27)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
