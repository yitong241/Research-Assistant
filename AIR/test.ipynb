{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = \"weather\"\n",
    "w2 = \"whether\"\n",
    "\n",
    "w1 = nlp(w1)\n",
    "w2 = nlp(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.similarity(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = nlp(\"I believe in the god of the Bible\")\n",
    "s2 = nlp('I trust in a higher power of Christianity')\n",
    "s3 = nlp(\"This week John will drink beer\")\n",
    "\n",
    "s1.similarity(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.similarity(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.similarity(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "kuaidi = []\n",
    "url='https://www.yueto.net/goods?value=257531390985'\n",
    "response = requests.get(url)\n",
    "response.encoding = 'gb18030' \n",
    "response = response.text\n",
    "soup = bs4.BeautifulSoup(response,'html.parser',from_encoding=\"utf8\")\n",
    "for i in soup.findAll(name='div',attrs = {'class':'timeline-item'}):\n",
    "    kuaidi.append(i.get_text())\n",
    "    print(i.get_text())\n",
    "print(kuaidi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "session = HTMLSession()\n",
    "url = 'https://www.yueto.net/goods?value=257531390985'\n",
    "sel = '#__layout > div > div > div.container > div:nth-child(3) > div > div > div:nth-child(3) > ul > li.black.timeline-item > div > div.detail > div'\n",
    "r = session.get(url)\n",
    "res = r.html.find(sel)\n",
    "#print(r.html.text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "session = HTMLSession()\n",
    "url = 'https://www.jianshu.com/p/85f4624485b9'\n",
    "sel = '#__next > div._21bLU4._3kbg6I > div > div._gp-ck > section:nth-child(1) > article > p:nth-child(4) > a'\n",
    "r = session.get(url)\n",
    "res = r.html.find(sel)\n",
    "#print(r.html.text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlpage =  'https://www.yueto.net/goods?value=257531390985'\n",
    "  \n",
    "HEADERS = ({'User-Agent':\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \\\n",
    "            (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\\\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "\n",
    "#page = urllib.request.urlopen(urlpage)\n",
    "page = requests.get(urlpage)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "print(soup)\n",
    "dom = etree.HTML(str(soup))\n",
    "print(dom.xpath('//*[@id=\"__layout\"]/div/div/div[4]/div[3]/div/div/div[2]/ul/li[1]'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import requests\n",
    "  \n",
    "  \n",
    "URL = \"https://en.wikipedia.org/wiki/Nike,_Inc.\"\n",
    "  \n",
    "HEADERS = ({'User-Agent':\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \\\n",
    "            (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\\\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "  \n",
    "webpage = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "dom = etree.HTML(str(soup))\n",
    "print(dom.xpath('//*[@id=\"firstHeading\"]')[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "#Function to Find the element from the Xpath\n",
    "def Xpath(url):\n",
    "  Dict_Headers = ({'User-Agent':\n",
    "      'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \\\n",
    "      (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\\\n",
    "      'Accept-Language': 'en-US, en;q=0.5'})\n",
    "  # Gets the requried data https browser's address bar\n",
    "  webPage = requests.get(url,Dict_Headers)\n",
    "  # Creating a soup Object from the html content\n",
    "  Scraping = BeautifulSoup(webPage.content, \"html.parser\") \n",
    "  # Conveting Soup object to etree object for Xpath processing\n",
    "  documentObjectModel = etree.HTML(str(Scraping)) \n",
    "  return (documentObjectModel.xpath('//*[@id=\"firstHeading\"]')[0].text)\n",
    "URL = \"https://en.wikipedia.org/wiki/Earth\"\n",
    "print(Xpath(URL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from PIL import Image\n",
    "from PIL import ImageChops\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def compare_images(path_one, path_two):\n",
    "    \"\"\"\n",
    "    比较图片\n",
    "    :param path_one: 第一张图片的路径\n",
    "    :param path_two: 第二张图片的路径\n",
    "    :return: 相同返回 success\n",
    "    \"\"\"\n",
    "    image_one = Image.open(path_one)\n",
    "    image_two = Image.open(path_two)\n",
    "    try:\n",
    "        diff = ImageChops.difference(image_one, image_two)\n",
    "\n",
    "        if diff.getbbox() is None:\n",
    "            # 图片间没有任何不同则直接退出\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    except ValueError as e:\n",
    "        return \"{0}\\n{1}\".format(e, \"图片大小和box对应的宽度不一致!\")\n",
    "\n",
    "def send(text):\n",
    "    url = 'https://maker.ifttt.com/trigger/delivery/json/with/key/wvZq6bXNphwSr3Z20twON'\n",
    "    payload = {\"value1\": text}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.request(\"POST\", url, data = json.dumps(payload), headers = headers)\n",
    "    print(response.text)\n",
    "text = \"没动静\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = 21\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')  # 设置option\n",
    "while True:\n",
    "    print(curr)\n",
    "    browser = webdriver.Chrome(options=option)\n",
    "    #browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.yueto.net/goods?value=257531390985\")\n",
    "    #print(browser.page_source)\n",
    "    browser.execute_script(\"document.body.style.zoom='30%'\")\n",
    "    print(\"sleep for 10 sec\")\n",
    "    time.sleep(10)\n",
    "    curr += 1\n",
    "    browser.save_screenshot(str(curr) + \".png\")\n",
    "    browser.close()\n",
    "    if not compare_images((str(curr-1)+\".png\"), (str(curr) + \".png\")):\n",
    "        send(text)\n",
    "    os.remove((str(curr-1)+\".png\"))\n",
    "    print(\"sleep for 30 sec\")\n",
    "    time.sleep(600) #睡10分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')  # 设置option\n",
    "\n",
    "browser = webdriver.Chrome(options=option)\n",
    "browser.get(\"https://www.yueto.net/goods?value=257531390985\")\n",
    "#print(browser.page_source)\n",
    "browser.execute_script(\"document.body.style.zoom='30%'\")\n",
    "time.sleep(10)\n",
    "browser.save_screenshot(\"0.png\")\n",
    "browser.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')  # 设置option\n",
    "\n",
    "browser = webdriver.Chrome(options=option)\n",
    "browser.get(\"https://sijipiao.fliggy.com/ie/flight_search_result.htm?searchBy=1281&spm=181.7091613.a1z67.1002&_input_charset=utf-8&tripType=0&depCityName=札幌&depCity=&depDate=2023-01-03&arrCityName=大阪&arrCity=OSA&arrDate=&ttid=seo.000000576\")\n",
    "#print(browser.page_source)\n",
    "browser.execute_script(\"document.body.style.zoom='50%'\")\n",
    "time.sleep(10)\n",
    "element = browser.find_element('link text', \"我知道了\")\n",
    "element.click()\n",
    "\n",
    "browser.save_screenshot(\"test.png\")\n",
    "browser.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this piece of code converts the raw csv file to processed input\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#读取news_data.csv，保存到新建的news_data.txt中\n",
    "data = pd.read_csv('earbud_selected_reviews.csv', encoding='utf-8')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "LL = []\n",
    "punctuation_string = string.punctuation # store the punctuation\n",
    "for line in data.values:\n",
    "    stri = str(line[13]) # this is the review body\n",
    "    L = stri.split(\" \") # split the review body by space\n",
    "    for word in L:\n",
    "        LL.append(porter.stem(word)) # append each word of each review body to LL\n",
    "    \n",
    "#print(LL)\n",
    "\n",
    "\n",
    "with open('earbud_selected_reviews.txt','a+', encoding='utf-8') as f: # store the data in this file\n",
    "    temp = 0 \n",
    "    for line in data.values:\n",
    "        if temp < 100:\n",
    "            stri = str(line[13])\n",
    "            for i in punctuation_string:\n",
    "                stri = stri.replace(i, '') # replace punctuation with nothing\n",
    "            stri = stri.lower() # convert to lower case\n",
    "            #str(line[0])：csv中第0列；+','+：csv两列之间保存到txt用逗号（，）隔开；'\\n'：读取csv每行后在txt中换行\n",
    "            L = stri.split(\" \") # split by space\n",
    "            filtered_words = [porter.stem(word) for word in L if (porter.stem(word) not in stopwords.words('english')) and (LL.count(word)) >= 10] # keep only if appear more than 10 times and not stop words\n",
    "            new = \" \".join(filtered_words)\n",
    "            #f.write(str(line[1]) + \" \" + str(line[2]) + \" \" + new + \" \" + stri + \" \" + str(line[8]) + \" \" + str(int(line[9]) - int(line[8])) + '\\n')\n",
    "            f.write(str(line[1]) + \"_\" + str(line[2]) + \" \" + str(line[7]) + \" \" + new + '\\n')\n",
    "            # line[1] is customer_id \n",
    "            # line[2] is review_id\n",
    "            \n",
    "        temp += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv  \n",
    "file = open(\"AIR_document_topic\")\n",
    "\n",
    "LL = []\n",
    "topic_score = file.readlines()\n",
    "'''\n",
    "file2 = open(\"earbud.csv\")\n",
    "\n",
    "info = file2.readlines()\n",
    "'''\n",
    "\n",
    "with open(\"earbud.csv\") as file2:\n",
    "    info = lines = [line.rstrip('\\n') for line in file2]\n",
    "\n",
    "file3 = open(\"AIR_score\")\n",
    "\n",
    "f3 = file3.readlines()\n",
    "\n",
    "for i in range(122462):\n",
    "    L = []\n",
    "    temp = topic_score[i].split(\" \")[0].split(\"\\t\")\n",
    "    t = info[i]\n",
    "    #t = info[i].split(\" \", 1)[1].split(\" \",2)\n",
    "    #t.insert(0, info[i].split(\" \", 1)[0])\n",
    "    tempp = [int(x) for x in temp]\n",
    "    summation = sum(tempp)\n",
    "\n",
    "\n",
    "    #scor = f3[i].replace(\"\\t\", \"\\n\", \" \")\n",
    "    score = f3[i][:-1].split(\"\\t\")\n",
    "    scoree = [float(x) for x in score][1:]\n",
    "\n",
    "    predicted_score = sum(scoree)/10\n",
    "\n",
    "    L.append(t[0])\n",
    "    L.append(t[1])\n",
    "    L.append(t[2])\n",
    "    L.append(str(predicted_score))\n",
    "    L.append(t[3])\n",
    "    L.append(t[4])\n",
    "    L.append(t[5])\n",
    "    L.append(t[6])\n",
    "\n",
    "\n",
    "    for num in tempp:\n",
    "        L.append(\"%.2f\" % (num/summation*100) + \"%\" + \"  \")\n",
    "    LL.append(L)\n",
    "\n",
    "with open('countries.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    writer.writerows(LL)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '5.0\t5.017\t5.003\t5.132\t5.197\t5.318\t4.981\t5.000\t4.993\t5.041\t5.000'\n",
    "score = a.split(\"\\t\")\n",
    "scoree = [float(x) for x in score][1:]\n",
    "print(sum(scoree)/10)\n",
    "print(scoree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"earbud_selected_reviews.txt\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('earbud.csv')\n",
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open(\"earbud.csv\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        if any(line):\n",
    "            count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"earbud.csv\", newline='') as in_file:\n",
    "    with open(\"out.csv\", 'w', newline='') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        for row in csv.reader(in_file):\n",
    "            if row:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_result.csv\") as f:\n",
    "    with open(\"earbud_selected_reviews.csv\") as fff:\n",
    "        with open(\"final_out.csv\", \"w\") as ff:\n",
    "            data = []\n",
    "            for line in csv.reader(fff):\n",
    "                data.append(line)\n",
    "            writer = csv.writer(ff)\n",
    "            for row in csv.reader(f):\n",
    "                L = []\n",
    "                customer_id = row[0].split(\"_\")[0]\n",
    "                review_id = row[0].split(\"_\")[1]\n",
    "                for roww in data:\n",
    "                    if roww[1] == customer_id and roww[2] == review_id:\n",
    "                        L.append(customer_id)\n",
    "                        L.append(review_id)\n",
    "                        L.append(row[1])\n",
    "                        L.append(row[2])\n",
    "                        L.append(row[3])\n",
    "                        L.append(roww[13])\n",
    "                        L.append(roww[8])\n",
    "                        L.append(str(int(roww[9]) - int(roww[8])))\n",
    "                        L.append(row[4])\n",
    "                        L.append(row[5])\n",
    "                        L.append(row[6])\n",
    "                        L.append(row[7])\n",
    "                        L.append(row[8])\n",
    "                        L.append(row[9])\n",
    "                        L.append(row[10])\n",
    "                        L.append(row[11])\n",
    "                        L.append(row[12])\n",
    "                        L.append(row[13])\n",
    "                        writer.writerow(L)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "# read contents of csv file\n",
    "file = pd.read_csv(\"final_out.csv\")\n",
    "print(\"\\nOriginal file:\")\n",
    "#print(file)\n",
    "  \n",
    "# adding header\n",
    "headerList = [\"customer_id\", \"review_id\", \n",
    "\"actual_rating\",\n",
    "\"predicted_rating\",\n",
    "\"processed_content\",\n",
    "\"original_content\",\n",
    "\"up_vote\",\n",
    "\"down_vote\",\n",
    "\"prob_a1\",\n",
    "\"prob_a2\",\n",
    "\"prob_a3\",\n",
    "\"prob_a4\",\n",
    "\"prob_a5\",\n",
    "\"prob_a6\",\n",
    "\"prob_a7\",\n",
    "\"prob_a8\",\n",
    "\"prob_a9\",\n",
    "\"prob_a10\"]\n",
    "\n",
    "\n",
    "  \n",
    "# converting data frame to csv\n",
    "file.to_csv(\"gfg2.csv\", header=headerList, index=False)\n",
    "  \n",
    "# display modified csv file\n",
    "file2 = pd.read_csv(\"gfg2.csv\")\n",
    "print('\\nModified file:')\n",
    "print(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AIR_document_topic\") as f:\n",
    "    with open(\"pp.csv\", \"w\", newline='') as ff:\n",
    "        writer = csv.writer(ff)\n",
    "        data = f.readlines()\n",
    "        for line in data:\n",
    "            line = line.split(\"\\t\")\n",
    "            writer.writerow(line)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headerList = [\n",
    "\"num_appearance_a1\",\n",
    "\"num_appearance_a2\",\n",
    "\"num_appearance_a3\",\n",
    "\"num_appearance_a4\",\n",
    "\"num_appearance_a5\",\n",
    "\"num_appearance_a6\",\n",
    "\"num_appearance_a7\",\n",
    "\"num_appearance_a8\",\n",
    "\"num_appearance_a9\",\n",
    "\"num_appearance_a10\"]\n",
    "\n",
    "file = pd.read_csv(\"pp.csv\")\n",
    "file.to_csv(\"gfg2.csv\", header=headerList, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy as en\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "a = [4, 4, 4, 4, 4, 9]\n",
    "a = pd.Series(a)\n",
    "data = a.value_counts()\n",
    "print(\"done\")\n",
    "en(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "def ent(data):\n",
    "    \"\"\"Calculates entropy of the passed `pd.Series`\n",
    "    \"\"\"\n",
    "    data = pd.Series(data)\n",
    "    p_data = data.value_counts()           # counts occurrence of each value\n",
    "    entropy = scipy.stats.entropy(p_data)  # get entropy from counts\n",
    "    return entropy\n",
    "\n",
    "a = [3,1]\n",
    "print(ent(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AIR_document_topic.csv\") as file:\n",
    "        all = [\"Entropy\"]\n",
    "        r = csv.reader(file)\n",
    "        header = next(r)\n",
    "\n",
    "        for row in r:\n",
    "            temp = []\n",
    "            for number in range(10):\n",
    "                app = int(row[number])\n",
    "                for i in range(app):\n",
    "                    temp.append(number)\n",
    "\n",
    "\n",
    "            entropy = ent(temp)\n",
    "            all.append(entropy)\n",
    "\n",
    "print(len(all))\n",
    "print(all)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "with open(\"air_document_topic.csv\") as file:\n",
    "        all = [\"Entropy\"]\n",
    "        r = csv.reader(file)\n",
    "        header = next(r)\n",
    "\n",
    "\n",
    "        for row in r:\n",
    "            temp = []\n",
    "            total = 0\n",
    "            #print(row)\n",
    "            for number in range(10):\n",
    "                total += int(row[number])\n",
    "                \n",
    "            for i in row:\n",
    "                temp.append(int(i)/total)\n",
    "\n",
    "            #entropy = ent(temp)\n",
    "            #print(temp)\n",
    "            ent = entropy(list(temp), base=2)\n",
    "            #print(ent)\n",
    "            all.append(ent)\n",
    "\n",
    "print(len(all))\n",
    "print(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_result_updated.csv\", newline='') as fdin, open(\"test.csv\", \"w\") as fdout:\n",
    "    count = 0\n",
    "    aa = []\n",
    "    for line in csv.reader(fdin):\n",
    "        line.append(all[count])\n",
    "        aa.append(line)\n",
    "        count += 1\n",
    "    writer = csv.writer(fdout)\n",
    "    writer.writerows(aa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122463\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "LL = [\"Review Length\"]\n",
    "with open(\"final_result_updated.csv\", newline='') as fdin:\n",
    "\n",
    "    for line in csv.reader(fdin):\n",
    "        string = line[5]\n",
    "        words = nltk.word_tokenize(string)\n",
    "\n",
    "        words=[word.lower() for word in words if word.isalpha()]\n",
    "        LL.append(len(words))\n",
    "\n",
    "print(len(LL))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.csv\", newline='') as fdin, open(\"testt.csv\", \"w\") as fdout:\n",
    "    count = 1\n",
    "    aa = []\n",
    "    for line in csv.reader(fdin):\n",
    "        line.append(LL[count])\n",
    "        aa.append(line)\n",
    "        count += 1\n",
    "    writer = csv.writer(fdout)\n",
    "    writer.writerows(aa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3372900666170139"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "a = [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0]\n",
    "b=  [0.0625, 0, 0.9375, 0, 0, 0, 0, 0, 0, 0]\n",
    "entropy(b, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
