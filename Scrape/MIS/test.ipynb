{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# csv_file_name = 'MIS_data.csv'\n",
    "\n",
    "# columns = [\"Title\", \"Authors\", \"Publish Date\", \"Abstract\", \"Keywords\", \"PDF Name\"]\n",
    "# with open(csv_file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.DictWriter(file, fieldnames=columns)\n",
    "#     writer.writeheader()\n",
    "\n",
    "# def add_row_to_csv(data):\n",
    "#     with open(csv_file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "#         writer = csv.DictWriter(file, fieldnames=columns)\n",
    "#         writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file_name = 'MIS_data.csv'\n",
    "\n",
    "columns = [\"Title\", \"Authors\", \"Publish Date\", \"Abstract\", \"Keywords\", \"PDF Name\"]\n",
    "\n",
    "def add_row_to_csv(data):\n",
    "    with open(csv_file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=columns)\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credentials():\n",
    "    with open('../credentials.txt', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        username = lines[0].strip()\n",
    "        password = lines[1].strip()\n",
    "        file.close()\n",
    "        return username, password\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_line(line_number, new_value):\n",
    "    with open('progress.txt', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    if 0 <= line_number < len(lines):\n",
    "        lines[line_number] = f\"{new_value}\\n\" \n",
    "        with open('progress.txt', 'w') as file:\n",
    "            file.writelines(lines)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def read_numbers():\n",
    "    with open('progress.txt', 'r') as file:\n",
    "        numbers = [int(line.strip()) for line in file]\n",
    "    return numbers[0], numbers[1], numbers[2], numbers[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import requests\n",
    "import time\n",
    "import urllib\n",
    "import os\n",
    "import shutil\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "Initial_path = r\"C:\\Users\\yitong\\Desktop\\Research-Assistant\\Scrape\\MIS\\PDF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_article(driver, wait):\n",
    "\n",
    "    # Function to find the following dd for a given dt text\n",
    "    def find_following_dd_by_dt_text(dt_text):\n",
    "        dt_elements = driver.find_elements(By.XPATH, \"//dl[@id='citationFields']/dt\")\n",
    "        for dt in dt_elements:\n",
    "            if dt.text.strip(':') == dt_text:\n",
    "                return dt.find_element(By.XPATH, \"./following-sibling::dd[1]\")\n",
    "        return None\n",
    "\n",
    "    # Extract Title\n",
    "    title_element = driver.find_element(By.CSS_SELECTOR, \"dd.citation-title > a > div > h1\")\n",
    "    title = title_element.text\n",
    "\n",
    "    # Extract Authors\n",
    "    authors_element = find_following_dd_by_dt_text(\"Authors\")\n",
    "    authors = \"; \".join([author.text for author in authors_element.find_elements(By.TAG_NAME, \"a\")])\n",
    "\n",
    "    # Extract Subject Terms\n",
    "    subject_terms_element = find_following_dd_by_dt_text(\"Subject Terms\")\n",
    "    subject_terms = \"; \".join([term.text for term in subject_terms_element.find_elements(By.TAG_NAME, \"a\")])\n",
    "\n",
    "    # Extract Publish Date (assuming it's within the Source information)\n",
    "    source_element = find_following_dd_by_dt_text(\"Source\")\n",
    "    publish_date = source_element.text.split('. ')[1].split(', ')[0]\n",
    "\n",
    "    # Extract Accession Number\n",
    "    accession_number_element = find_following_dd_by_dt_text(\"Accession Number\")\n",
    "    accession_number = accession_number_element.text\n",
    "\n",
    "    abstract_element = find_following_dd_by_dt_text(\"Abstract\")\n",
    "    abstract = abstract_element.text\n",
    "\n",
    "    add_row_to_csv({\n",
    "        \"Title\": title,\n",
    "        \"Authors\": authors,\n",
    "        \"Publish Date\": publish_date,\n",
    "        \"Abstract\": abstract,\n",
    "        \"Keywords\": subject_terms,\n",
    "        \"PDF Name\": accession_number\n",
    "    })\n",
    "\n",
    "    pdf_link_element = driver.find_element(\"link text\", 'PDF Full Text')\n",
    "    pdf_link_element.click()\n",
    "    wait.until(EC.element_to_be_clickable((By.ID, \"downloadLink\")))\n",
    "    pdf_download_element = driver.find_element(By.ID, \"downloadLink\")\n",
    "    pdf_download_element.click()\n",
    "    time.sleep(10)\n",
    "    filename = max([Initial_path + \"\\\\\" + f for f in os.listdir(Initial_path)], key=os.path.getctime)\n",
    "    shutil.move(filename, os.path.join(Initial_path, accession_number + \".pdf\"))\n",
    "    window_handles = driver.window_handles\n",
    "    driver.switch_to.window(window_handles[0])\n",
    "    time.sleep(5)\n",
    "    driver.execute_script(\"window.history.go(-1)\")\n",
    "    time.sleep(10)\n",
    "    driver.execute_script(\"window.history.go(-1)\")\n",
    "    time.sleep(10)\n",
    "    print(\"back to page page\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先处理这个年份这个issue剩下的article\n",
    "# 再处理这个年份剩下的issue\n",
    "# 再处理剩下的年份\n",
    "\n",
    "def recover_from_breakpoint(year, issue, page, article):\n",
    "    \n",
    "    user_id, password = get_credentials()\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('prefs', {\n",
    "        \"download.default_directory\": Initial_path,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"plugins.always_open_pdf_externally\": True \n",
    "    })\n",
    "\n",
    "    wd = webdriver.Chrome(service = Service(r'../chromedriver.exe'), options=options)\n",
    "    wait = WebDriverWait(wd, 60)\n",
    "    \n",
    "    wd.get(\"https://libproxy1.nus.edu.sg/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bsu&jid=MIS&site=ehost-live\")\n",
    "    button = wd.find_element(By.ID, 'btn_nus')\n",
    "    button.click()\n",
    "    wait.until(EC.element_to_be_clickable((By.ID, 'userNameInput')))\n",
    "    user_id_box = wd.find_element(By.ID, 'userNameInput')\n",
    "    password_box = wd.find_element(By.ID, 'passwordInput')\n",
    "    user_id_box.send_keys(user_id)\n",
    "    password_box.send_keys(password)\n",
    "    submit_button = wd.find_element(By.ID, 'submitButton')\n",
    "    submit_button.click()\n",
    "    wait.until(EC.element_to_be_clickable((By.CLASS_NAME, \"osano-cm-accept-all\")))\n",
    "    cookie_button_element = wd.find_element(By.CLASS_NAME, \"osano-cm-accept-all\")\n",
    "    cookie_button_element.click()\n",
    "\n",
    "    year_xpath = \"//td[@class='authVolIssue_volume_cell']/a[contains(@href, 'javascript:ToggleVolumeIssue')]\"\n",
    "    wait.until(EC.element_to_be_clickable((By.XPATH, year_xpath)))\n",
    "    year_elements = wd.find_elements(By.XPATH, year_xpath)\n",
    "    for j in range(year, len(year_elements)):\n",
    "        year_elements[j].click()\n",
    "        time.sleep(3)\n",
    "        issue_xpath = \"//a[@data-auto='volume_issue_issue_link' and contains(@href, 'XslPostBack')]\"\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, issue_xpath)))\n",
    "        issue_elements = wd.find_elements(By.XPATH, issue_xpath)\n",
    "        for i in range(issue, len(issue_elements)):\n",
    "            issue_elements[i].click()\n",
    "            time.sleep(5)\n",
    "            if page == 1:\n",
    "                title_elements = wd.find_elements(By.CSS_SELECTOR, \"a.title-link.color-p4\")\n",
    "                for k in range(article, len(title_elements)):\n",
    "                    title_elements[k].click()\n",
    "                    print(year, issue, page, k)\n",
    "                    time.sleep(5)\n",
    "                    get_one_article(wd, wait)\n",
    "                    window_handles = wd.window_handles\n",
    "                    wd.switch_to.window(window_handles[0])\n",
    "                    title_elements = wd.find_elements(By.CSS_SELECTOR, \"a.title-link.color-p4\")\n",
    "                    overwrite_line(3, k + 1)\n",
    "                page = 2\n",
    "                article = 0\n",
    "                overwrite_line(2, page)\n",
    "                overwrite_line(3, 0)\n",
    "                # raise Exception\n",
    "\n",
    "            if page == 2:\n",
    "                page_element = wd.find_elements(By.XPATH, \"//a[@title='Skip to page 2']\")\n",
    "                page_element[0].click()\n",
    "                title_elements = wd.find_elements(By.CSS_SELECTOR, \"a.title-link.color-p4\")\n",
    "                for m in range(article, len(title_elements)):\n",
    "                    title_elements[m].click()\n",
    "                    print(year, issue, page, m)\n",
    "                    time.sleep(5)\n",
    "                    get_one_article(wd, wait)\n",
    "                    window_handles = wd.window_handles\n",
    "                    wd.switch_to.window(window_handles[0])\n",
    "                    title_elements = wd.find_elements(By.CSS_SELECTOR, \"a.title-link.color-p4\")\n",
    "                    overwrite_line(3, m + 1)\n",
    "\n",
    "                page_element = wd.find_elements(By.XPATH, \"//a[@title='Skip to page 3']\")\n",
    "                if len(page_element) == 0:\n",
    "                    # This issue only has 2 pages\n",
    "                    page = 1\n",
    "                    article = 0\n",
    "                    overwrite_line(2, page)\n",
    "                    overwrite_line(3, 0)\n",
    "                    page_element = wd.find_elements(By.XPATH, \"//a[@title='Skip to page 1']\")\n",
    "                    page_element[0].click()\n",
    "                    print(\"back to page 1\")\n",
    "                    break\n",
    "                \n",
    "                page = 3\n",
    "                article = 0\n",
    "                overwrite_line(2, page)\n",
    "                overwrite_line(3, 0)\n",
    "\n",
    "            if page == 3:\n",
    "                page_element = wd.find_elements(By.XPATH, \"//a[@title='Skip to page 3']\")\n",
    "                page_element[0].click()\n",
    "                title_elements = wd.find_elements(By.CSS_SELECTOR, \"a.title-link.color-p4\")\n",
    "                for n in range(article, len(title_elements)):\n",
    "                    title_elements[n].click()\n",
    "                    print(year, issue, page, n)\n",
    "                    time.sleep(5)\n",
    "                    get_one_article(wd, wait)\n",
    "                    window_handles = wd.window_handles\n",
    "                    wd.switch_to.window(window_handles[0])\n",
    "                    title_elements = wd.find_elements(By.CSS_SELECTOR, \"a.title-link.color-p4\")\n",
    "                    overwrite_line(3, n + 1)\n",
    "                page_element = wd.find_elements(By.XPATH, \"//a[@title='Skip to page 1']\")\n",
    "                page_element[0].click()\n",
    "                print(\"back to page 1\")\n",
    "\n",
    "            page = 1\n",
    "            article = 0\n",
    "            overwrite_line(2, page)\n",
    "            overwrite_line(3, 0)\n",
    "            overwrite_line(1, i + 1)\n",
    "            wd.get(\"https://libproxy1.nus.edu.sg/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bsu&jid=MIS&site=ehost-live\")\n",
    "            time.sleep(10)\n",
    "            print(\"back to issue page\")\n",
    "            year_elements = wd.find_elements(By.XPATH, year_xpath)\n",
    "            year_elements[j].click()\n",
    "            time.sleep(5)\n",
    "            issue_elements = wd.find_elements(By.XPATH, issue_xpath)\n",
    "        issue = 0\n",
    "        overwrite_line(1, 0)\n",
    "        overwrite_line(0, j + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=122.0.6261.95)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6C1905E42+3538674]\n",
      "\t(No symbol) [0x00007FF6C1524C02]\n",
      "\t(No symbol) [0x00007FF6C13D5AEB]\n",
      "\t(No symbol) [0x00007FF6C13B288C]\n",
      "\t(No symbol) [0x00007FF6C1445DD7]\n",
      "\t(No symbol) [0x00007FF6C145B40F]\n",
      "\t(No symbol) [0x00007FF6C143EE53]\n",
      "\t(No symbol) [0x00007FF6C140F514]\n",
      "\t(No symbol) [0x00007FF6C1410631]\n",
      "\tGetHandleVerifier [0x00007FF6C1936CAD+3738973]\n",
      "\tGetHandleVerifier [0x00007FF6C198C506+4089270]\n",
      "\tGetHandleVerifier [0x00007FF6C1984823+4057299]\n",
      "\tGetHandleVerifier [0x00007FF6C1655C49+720121]\n",
      "\t(No symbol) [0x00007FF6C153126F]\n",
      "\t(No symbol) [0x00007FF6C152C304]\n",
      "\t(No symbol) [0x00007FF6C152C432]\n",
      "\t(No symbol) [0x00007FF6C151BD04]\n",
      "\tBaseThreadInitThunk [0x00007FFFDAE0257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFFDCE6AA58+40]\n",
      "\n",
      "Failed\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        year, issue, page, article = read_numbers()\n",
    "        recover_from_breakpoint(year, issue, page, article)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
